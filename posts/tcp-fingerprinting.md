title: TCP Service Fingerprinting with Denied HTTP Requests and Other Timing Sidechannels
published: 2024-05-23
extract: Exploiting behaviours outside 
***Note:* Unfortunately, before publishing this (but after making a test site, and performing some experiments showing it did once work), it appears most major browsers have patched the CORS fingerprinting technique. You can still try the site [here](https://pbeart.github.io/daktiloskopija/) but it isn't likely to work. I have rewritten this post to try to appeal to a less technical audience, and to provide some broader insight into sidechannel-type vulnerabilities in general.**

[CORS (Cross-Origin Resource Sharing)](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS) is a web-browser security mechanism which aims to prevent web requests being made to unauthorised resources. This includes situations such as malicious websites doing things to your logged-in Facebook using stored authentication cookies, or configuring your home Wi-Fi. You don't need to know how it works to read this post; all you need to know is the interesting detail that **CORS uses information received from accessing the target server to decide whether to allow access to it.** This may seem like a counterintuitive property, but it's essential to the purpose that CORS serves: it allows target websites to decide whether they want to be able to be requested by other sites, by setting certain headers in the response. In the case of sites like Facebook, this is almost certainly not going to be the case, but for services such as a weather API used from many other websites it's crucial.

## Checking what you can do after you do it
The browser will first make a request to the target server. If the CORS policy the browser receives from the target server turns out to mean we **shouldn't** have been able to make the request, then the browser behaves as though the request failed: it does not allow any returned data to be accessed by the script (the part whose maliciousness the mechanism is trying to defend against) which requested it. There's an interesting similarity with a CPU feature called [speculative execution](https://en.wikipedia.org/wiki/Speculative_execution) here, and if you have heard of it then you may already be able to imagine where this is going.

Modern CPUs have a feature called *branch prediction*, which essentially means they try to avoid the long delay required to fetch the next instruction from memory by fetching what they think is **likely** to be the next instruction (useful when that answer is more complicated than "the one after this one"!), and a feature called *speculative execution* takes it a step further: not only does it fetch a likely next instruction, it actually executes (runs) it. As with CORS, if it turns out to be wrong (for example if a branch went the other way than it expected), it "undoes" the effects, and returns the processor back to how it was, before loading and executing the correct instruction.

## All is specified to be well
In both cases, we might naïvely assume that the "nothing" which the specification says happens if we weren't supposed to have done something is really "nothing" happening. It's true that in the CORS case, the response is the same as if the request failed, and in the CPU case, the processor state after a failed speculation is the same as if the speculation hadn't occurred, which is why the issue in both cases has to do with a *sidechannel*: an "out-of-band" aspect of the system, not specified by documentation, and in both cases the sidechannel is the common one of **timing**. Timing is often a highly variable and performance-dependent aspect of a system, which might be considered to vary almost randomly, but in both of these cases it is affected by a factor **we should not have knowledge of.**

## Not-quite nothing

In the web request case, if the request we make really has no server to answer it, the request will generally time-out, that is, give up and not wait forever for a reply. If the request fails due to CORS however, **the browser may cause the request to time-out as soon as it knows it cannot succeed**. What this means in practice is, although the response in both cases is a failure, it will be a quicker failure if something is actually there and simply does not want to talk to us, and we can use this difference in time to **determine the existence of things we shouldn't be able to access at all**. Practically, this means we can identify things like programs running on a user's computer and their local network, which either do not answer HTTP requests properly or answer with failure-causing CORS policies, but whose response nonetheless allows us to determine their existence, and to build a profile of the user.

For the CPU case, it is a little more complex, but with a far higher potential payoff. I won't go into any significant detail here, but essentially by getting the CPU to speculatively run some instructions we shouldn't be allowed to, we can extract information on a secret, **even though the CPU later un-does all the apparent consequences of reading it**. A common technique is to make the speculative code read a single bit of some secret, then load one of two memory locations depending on the result, which causes subsequent loads to that memory location to be faster, as the CPU cache now keeps a copy of the one we ended up loading, and the CPU does not undo this when the speculated instructions turn out to have been forbidden. Vulnerabilities following this pattern (transient execution vulnerabilities) were initially widely exploited in the form of [Spectre and Meltdown](https://www.cloudflare.com/learning/security/threats/meltdown-spectre/) in 2017 and 2018.

## Lessons

The obvious question is, what can be done about these vulnerabilities, and the issue in general? These vulnerabilities exist by definition outside what is specified in the behaviour of the systems which demonstrate them, and so typical ways to systematically such problems are likely to fail. The solutions, much like the attacks themselves, become statistical bodge-jobs, modifying the behaviour to decouple the sidechannel from sensitive information, like making CORS-failing requests take artificially longer or randomly varied amounts of time, or impairing speculation and caching features.

The real thing to take away from all this, in my opinion, is something rather fundamental about computer security, which is the repeated failure of attempts to avoid vulnerabilities exclusively through "justifiably-correct" design, without adequate testing or realistic thinking. It's an excellent idea to have a rigorously defined way for a system to be secure, and provable correctness is making gains as an approach for both software and hardware development, but without doing the boring, manual and fudge-y testing and repeated improvement of a product, your theoretical model will by definition not be able to predict the unconsidered issue, which often takes some scrappy thinking. There might be a good analogy here with cryptography, where anyone who invents a new algorithm is very likely to have a convincing argument for why it is secure, but the industry is still extremely reluctant to trust any new algorithm without actual large-scale testing, preferably in the real world. ⚫︎